import axios from "axios";
import dotenv from "dotenv";
dotenv.config();

const HF_API_URL = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"; 
const HF_API_KEY = process.env.HUGGINGFACE_API_KEY;

export async function getAIReply(fullContext) {
  try {
    console.log("üß† Sending message to local Ollama model...");

    // ‚ú® Format the prompt with role-based memory (natural chat style)
    const prompt = `
You are Jarvis, an intelligent, kind, and witty WhatsApp assistant.
You always reply in a natural, conversational tone ‚Äî short, clear, and human-like.
Use emojis occasionally but not excessively.

Here‚Äôs the conversation so far:
${fullContext}

Now respond as Jarvis to the last user message.
`;

    // üöÄ Send to local Ollama model
    const response = await axios.post("http://localhost:11434/api/generate", {
      model: "phi3",
      prompt,
      stream: false, // optional: disable chunked streaming for simplicity
    });

    // ‚úÖ Ollama responses come in `response.data.response`
    const reply = response.data?.response?.trim();

    if (!reply) {
      console.warn("‚ö†Ô∏è No valid reply from Ollama.");
      return "Hmm... I didn‚Äôt quite catch that. Could you repeat?";
    }

    return reply;
  } catch (error) {
    console.error("‚ùå Error communicating with Ollama:", error.message);
    return "‚ö†Ô∏è Sorry, my brain (local model) is unavailable right now.";
  }
}